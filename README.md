# ğŸ¤ Voice Tutor â€“ Open-Source, Offline Educational Assistant Powered by Local LLM

<div align="center">

![Python](https://img.shields.io/badge/python-v3.8+-blue.svg)
![Streamlit](https://img.shields.io/badge/streamlit-latest-red.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)
![Ollama](https://img.shields.io/badge/ollama-compatible-orange.svg)
![Offline](https://img.shields.io/badge/100%25-offline-purple.svg)

**A 100% offline, open-source, voice-driven tutor that's free, flexible, and aligned with the Victorian Curriculum Fâ€“10 Version 2.0**

</div>

---

## ğŸŒŸ Overview

While tech giants pour AI into enterprise toolsâ€”think automated analytics or chatbotsâ€”let's talk about something more grounded: helping kids learn when life's too busy for one-on-one teaching. **Voice Tutor** is the answerâ€”a **100% offline, open-source, voice-driven tutor** that's free, flexible, and aligned with the **Victorian Curriculum Fâ€“10 Version 2.0**. It's not a locked-down app that costs as much as a new phone. It's a build-it-yourself kit, ready for you to tweak, break, and learn from.

Voice Tutor supports both **voice** and **text-based** interactions, allowing learners to speak or type their questions and receive responses in **voice and text**. It's designed to be engaging, hands-free, and accessible for all.

## ğŸš€ Features

- ğŸ¤ **Voice Input**: Ask questions by speaking, powered by speech recognition
- ğŸ’¬ **Chat Input**: Type queries for a traditional chat experience
- ğŸ—£ï¸ **Voice Output**: Answers are read aloud using text-to-speech (TTS)
- ğŸ“ƒ **Text Output**: Responses are displayed on screen for clarity and review
- ğŸ“ **Curriculum-Aligned**: Pulls from a knowledge base tied to the Victorian Curriculum Fâ€“10 Version 2.0
- ğŸ’» **100% Offline**: Runs locally with no internet dependency
- ğŸ› ï¸ **Open-Source**: Free to use, modify, and extend
- ğŸ”Š **Multilingual Support**: Optional voice model expansions for different languages
- ğŸ“š **Interactive Lessons**: Lessons designed for an engaging learning experience

## ğŸ—ï¸ System Architecture

```mermaid
graph TD
    A[User ğŸ‘¤]
    
    subgraph Input
        A -->|Voice| B[Microphone ğŸ¤]
        A -->|Text| C[UI: Text Input âŒ¨ï¸]
        B --> D[Speech-to-Text ğŸ§]
        D --> E[Query Processor âš™ï¸]
        C --> E
    end
    
    subgraph Processing
        E --> F[Ollama LLM ğŸ§ ]
        F --> G[voicetutor_db.json ğŸ’¾]
        F --> H[syllabus.json ğŸ“š]
    end
    
    subgraph Output
        F --> I[Response Formatter ğŸ”§]
        I --> J[UI: Display ğŸ–¥ï¸]
        I --> K[Text-to-Speech ğŸ”Š]
        K --> L[Speakers ğŸ¶]
        J --> A
        L --> A
    end
```

### ğŸ“‹ Architecture Explanation

- **ğŸ‘¤ User**: Interacts via voice (microphone) or text (UI input field)
- **ğŸ§ Speech-to-Text Module**: Converts voice input to text using offline speech recognition
- **âŒ¨ï¸ UI: Text Input Field**: Accepts typed queries from the user
- **âš™ï¸ Query Processor**: Receives text from either input method and sends it to the Ollama LLM
- **ğŸ§  Ollama LLM**: Processes queries offline using a local language model, accessing:
  - `voicetutor_db.json`: General knowledge base
  - `syllabus.json`: Curriculum data aligned with the Victorian Curriculum Fâ€“10 Version 2.0
- **ğŸ”§ Response Formatter**: Prepares the LLM's response for both text and voice output
- **ğŸ”Š Text-to-Speech Module**: Converts text responses to audio for hands-free feedback
- **ğŸ–¥ï¸ UI: Display**: Shows text responses on the screen
- **ğŸ¶ Speakers/Headphones**: Plays audio responses for the user

This design ensures a fully offline, flexible, and accessible system for voice and text-based learning.

## ğŸ§  How It Works

| Mode | Description | Icon |
|------|-------------|------|
| ğŸ¤ **Voice Input** | Speak through a microphone; speech-to-text processes the query | ğŸ¤ |
| ğŸ’¬ **Text Input** | Type questions directly into the interface | ğŸ’¬ |
| ğŸ—£ï¸ **Voice Output** | Responses are read aloud via TTS for hands-free learning | ğŸ—£ï¸ |
| ğŸ“ƒ **Text Output** | Answers are shown on screen for accessibility and reference | ğŸ“ƒ |

## ğŸ“Œ Why Voice Tutor?

- ğŸ§â€â™‚ï¸ **Accessible**: Caters to diverse learning needs, including auditory and visual learners
- ğŸ§  **Curriculum-Focused**: Aligned with the Victorian Curriculum Fâ€“10 Version 2.0 for relevant, structured learning
- ğŸ” **Flexible**: Switch between voice and text inputs seamlessly
- ğŸ“± **Hands-Free**: Ideal for multitasking or visually impaired users
- ğŸŒ **Offline & Free**: No subscriptions, no internet, no hidden costs
- ğŸ”§ **Hackable**: Open-source for anyone to customize or enhance
- ğŸ“… **Time-Saving**: Useful for busy schedules, no need for constant internet connectivity

## ğŸ› ï¸ Installation

### ğŸ“‹ Prerequisites

- ğŸ Python 3.8+
- ğŸ¤– Ollama (for local LLM inference)
- ğŸ¤ Microphone for voice input
- ğŸ”Š Speakers or headphones for voice output

### âš™ï¸ Setup Steps

1. **ğŸ“‚ Clone the repository**:

   ```bash
   git clone https://github.com/Logulokesh/voice-tutor.git
   cd voice-tutor
   ```

2. **ğŸ§ª Set up a virtual environment**:

   ```bash
   python3 -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **ğŸ“¦ Install dependencies**:

   ```bash
   pip install -r requirements.txt
   ```

4. **ğŸ¤– Install Ollama**:

   - Voice Tutor uses **Ollama** for local language model inference

   - Download and install Ollama from [ollama.ai](https://ollama.ai)

   - Pull a model (e.g., `llama3`):

     ```bash
     ollama pull llama3
     ```

   - Ensure Ollama is running locally:

     ```bash
     ollama run llama3
     ```

## â–¶ï¸ How to Run

1. **ğŸš€ Start the Ollama server**:

   ```bash
   ollama run llama3
   ```

2. **ğŸ¯ Launch Voice Tutor**:

   ```bash
   streamlit run ui.py
   ```

3. **ğŸ¤ Interact** using voice or text through the interface

## ğŸ“ Project Structure

| File | Description | Icon |
|------|-------------|------|
| `core_tutor.py` | Core logic for voice/text processing and interaction | âš™ï¸ |
| `ui.py` | Handles the user interface | ğŸ–¥ï¸ |
| `syllabus.json` | Structured data aligned with the Victorian Curriculum Fâ€“10 Version 2.0 | ğŸ“š |
| `voicetutor_db.json` | Offline knowledge base for responses | ğŸ“‚ |
| `requirements.txt` | Python dependencies | ğŸ“‘ |
| `readme.txt` | Legacy reference info | ğŸ“„ |
| `.gitignore` | Excludes virtual env and other unwanted files | ğŸš« |

## ğŸ› ï¸ Tech Stack

- **ğŸ Python 3.8+**: Core programming language
- **ğŸ–¥ï¸ Streamlit**: Web interface framework
- **ğŸ¤– Ollama**: Local LLM inference engine
- **ğŸ¤ Speech Recognition**: Voice input processing
- **ğŸ”Š Text-to-Speech**: Voice output generation
- **ğŸ“š JSON**: Knowledge base storage format

## ğŸ¤ Contributing

We welcome pull requests, feedback, and ideas! Let's make learning more accessible and natural together. ğŸŒŸ

### ğŸ”§ How to Contribute

1. ğŸ´ Fork the repository
2. ğŸŒ¿ Create a feature branch (`git checkout -b feature/amazing-feature`)
3. âœ… Commit your changes (`git commit -m 'Add amazing feature'`)
4. ğŸš€ Push to the branch (`git push origin feature/amazing-feature`)
5. ğŸ“¬ Open a Pull Request

## ğŸ“œ License

Licensed under the **MIT License**. Free to use, modify, and distribute.

## ğŸ–¼ï¸ Screenshots

<div align="center">

### ğŸ§‘â€ğŸ« Main Interface

![VoiceTutor UI 001](screenshots/001%20-%20VoiceTutor%20Online%20Classroom%20-%20%5Blocalhost%5D.png)

### ğŸ’¬ Interactive Learning

![VoiceTutor UI 002](screenshots/002%20-%20VoiceTutor%20Online%20Classroom%20-%20%5Blocalhost%5D.png)

### ğŸ¤ Voice Interaction

![VoiceTutor UI 003](screenshots/003%20-%20VoiceTutor%20Online%20Classroom%20-%20%5Blocalhost%5D.png)

### ğŸ“š Curriculum Content

![VoiceTutor UI 005](screenshots/005%20-%20VoiceTutor%20Online%20Classroom%20-%20%5Blocalhost%5D.png)

### ğŸ¯ Learning Dashboard

![VoiceTutor UI 006](screenshots/006%20-%20VoiceTutor%20Online%20Classroom%20-%20%5Blocalhost%5D.png)

</div>
